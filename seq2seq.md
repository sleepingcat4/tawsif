## A Study of Seq2Seq Deep Learning Model (Architecture-wise)

![image](https://github.com/sleeping4cat/sleeping-thoughts/assets/112309211/585daa70-da3b-4aaf-a7bb-0170fae96946)

### Introduction

Seq2Seq model is often considered a foundational architecture that radically changed Machine Translation (MT). It came around 2014 from Illya (co-founder of OpenAI) while he was still working at Google. It was a groundbreaking innovation that shifted focus from Statistical Machine Translation (SMT) to Neural Machine Translation (NMT).

**Purpose of this article:** Describe the core architecture of Seq2Seq, offering a beginner-friendly introduction yet enjoyable for experts, with a mind-blowing realization of how simple models change the world.

Let’s begin!

### Architecture

Illya describes in his paper that Recurrent Neural Networks (RNNs) had previously shown promising results in taking a sequence and translating it into another target sequence. While it is possible to use RNNs, Long Short-Term Memory (LSTM) models are capable enough to translate between long sequences and target sequences.

Stretching further, the temporal dependencies and memory nature of long sequences (which can be DNA, for example, or words in a sentence) are not ideally suited for RNNs (though not impossible), but LSTMs handle them with ease because they can maintain long temporal dependencies.

I will focus on temporal dependencies. These refer to grammatical, linguistic, and other relationships between words in a sentence. Linguistically, it's a big jargon, so we won’t delve into that since our focus is on Deep Learning, haha!

Temporal dependencies are sometimes known as Dependency Parsing by some folks. Anyways, this was the core reason Illya took upon the task to leverage LSTM models.

Leveraging LSTM, Illya and his team created an encoder-decoder model. The encoder LSTM converts words in a sentence into a "context vector," and the decoder LSTM takes that vector and converts it back into text in the target sequence (e.g., from English to French).

#### Getting Technical

What happens: The LSTM model estimates the conditional probability from a source sequence to a target sequence, where the sequence size \( T \) might differ between the pairs.

To obtain the target sequence, the final hidden state from the encoder (dimensional representation \( v \)) is used to predict the target sequence.

In Illya’s case, he used the LSTM-LM formulation to calculate the final hidden state (encoder part) and leveraged Graves' LSTM formulation to find the source sequence distribution in the decoder part.

### What’s the Unique Aspect?

**Special Character:** Illya introduced a "special character" in his paper, “<EOS>.” In layman terms, it allows the model to understand the end of a sequence without being dependent on any predefined size. This was a very novel solution.

We’re almost near the end. What’s remaining?

### BLEU Metric

The BLEU metric evaluates how accurately certain MT methods perform translation from the source to the target language. We won’t discuss the BLEU metric in detail here, but Statistical Machine Translation (SMT) achieved a score of 33.3, while Seq2Seq achieved 34.8. Quite remarkable.

Now, if you’re a contender who believes SMT is outdated, hold your breath; SMT is equally good.

### Anything Missed?

Yes, we missed discussing Illya’s magic. In one instance, he reversed the words in source sentences (leaving target sentences unchanged) and achieved almost a 6-point increase in decoded translations. WOW!

This phenomenon, discovered as "minimal time lag," showed that reversing the source sentences didn’t change the average distance between the words in the two sentence pairs. However, the first few words of the source sentences were very close to the first few words of the target sentences, resulting in an improved BLEU metric.

> ### Comments
> I personally enjoyed reading the paper and refreshing my memory. I’ve encountered naive approaches to explaining the Seq2Seq model before on Medium, YouTube, and LinkedIn, as many explanations are plagiarized from other YouTubers, Keras tutorials, or Github code. Not the original approach.
> 
> Reading and understanding from the source can make all the difference in learning AI. I hope that from this article, people take away a deeper understanding of this foundational deep learning model that eventually gave birth to Transformers.

#### Code

**I found this wonderful repository:** [Seq2Seq Repository](https://github.com/farizrahman4u/seq2seq)

**Original Paper:** [Seq2Seq Paper](https://paperswithcode.com/method/seq2seq)
