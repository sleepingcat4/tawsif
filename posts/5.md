<img width="870" height="341" alt="image" src="https://github.com/user-attachments/assets/ed9ea3ef-0492-4438-8430-d15af9d8a69f" />


### Introduction
AlexNet is a monumental paper and architecture in the entire Artificial Intelligence field and especially in Computer Vision. Nowadays we have papers from MIT, Stanford and Google outlining extreme technical prowess but hardly any of those papers come close to the advances that AlexNet had brought-up in the field.

Common misconception: AlexNet was a 60 million parameters model and utilised dropout and Normalization layers to achieve SOTA. Won’t disagree that was my initial thought as well. Unfortunately that’s the most wrong idea someone can have about AlexNet.

Because AlexNet’s SOTA comes from each individual step that illya and Alex took throughout the paper. From preparing the dataset to using multiple GPUs. NO, I’m not talking about cuda. How having multiple GPUs for distributed learning revealed interesting characteristics was monumental in Artificial Intelligence as a whole.

In this piece, I will explain why I prefer learning AlexNet over all the other SOTA models’ papers.

### Data Processing
We have a tendency to apply as many filters, augmentations and techniques written in the textbook while coming to process our datasets. Without considering how they affect our data and what outcome we want to achieve. Obviously a common norm: we want to prevent overfitting but the question being is our method of slamming multiple transformations on the dataset effective?

Most of the Keras and Pytorch implementations and training notebooks available online slam uncountable numbers of transformations. Which is extremely counterproductive and here shines AlexNet’s prowess.

In AlexNet, authors outline few key data processing techniques (so basic can’t imagine)

- Downsampling the dataset (224X224)
- Training on Centred raw RGB values
- Subtracting mean activity over the dataset for each pixel value
- Augmentation Technique
- Random Patch extraction
- Altering RGB intensities using PCA

Quite interesting right! Because I never read papers before AlexNet which took such an interesting approach. Especially the augmentation techniques: AlexNet outlines if it was not taken it would result in overfitting and prompting to use smaller networks.

### Preprocessing steps
Downsampling was required to address the past hardware limitations of training a neural network model.

Training on centred raw RGB values: I’m not sure what benefit it served but I presume feeding the Convolutional network with the most relevant features.

Papers does not explain further about subtracting mean activity either.

##### Augmentation steps
Random training patch extraction from the original images resulted in unique new images by a factor of 2048. Remarkable! Unfortunately a drawback, it made the new image interdependent. (AlexNet mentions it)

Altering RGB was the most interesting technique presented in the paper. Because the author writes, it reduces the top-1% error rate by 1 percent. Which I didn’t read in past papers.

###### Actual Architecture
Yes, if you want a summary:

It’s a 60 million parameters model with 5 convolutional layers (2D Conv) and 3 learned fully connected layers. But, it’s more interesting than this.

Become a member
AlexNet utilises activation functions like ReLU, Local Response Normalisation and Overlap pooling. Further, it uses two GPUs to run different kernels on different GPUs and stitch them together to receive a columnar architecture.

Yes, you heard it correctly. AlexNet is not a linear Architecture. I bet something most individuals reading this article didn’t know before.

*Key pointers*

- ReLU normalization
- Local Response Normalization
- Overlap Pooling

ReLU Normalization: AlexNet argues, saturating gradient activation functions: TanH is equally a good choice for training a large scale image classification model (e.g. AlexNet). Unfortunately to reach 25% error rate with TanH requires significant number of epochs/iterations compare to ReLU (non-saturating activation functions)

##### Personal Quote

> ReLU was a speed choice not a technical advantage in AlexNet!

**Local Response Normalization**

It represents a competition mechanism exhibited by real neurons within the kernels. Lowering the error rate by 2%.

**Overlap Pooling**

Overlap pooling (MaxPool) reduced the error rate by 0.4%.

#### Faking multiple models using Dropout
Most online literature and university courses outline: Dropout aids in lowering overfitting in the model. By dropping random neurons.

I won’t disagree. AlexNet authors have a different explanation. OpenAI’s obsession with multiple models to reach SOTA ain’t new. AlexNet argues, if we wanted to reach SOTA multiple models to reach classification was the clear method.

Unfortunately, training multiple models was time-consuming. Which is why they used Dropout layer to drop random neurons from the architecture and present us with a new architecture each inference time.

In simple terms: By dropping neurons in the architecture but letting the neurons within the architecture share the same weights, we get the SOTA predictions from different architectures each time and resulting robust prediction.

#### Details of learning
Weight decay was a key component. Because it reduced the error rate presented in the training period significantly.

**Cross-Validation:** Utilising how the kernels between different GPUs are combined acted as a cross-validation step.

**Split of Dataset:** A random 50–50 training and test dataset split was done.

#### Prowess of using Multiple GPUs

Stitching kernels together from multiple GPUs, helped AlexNet authors to have two distinctive models (one for each GPU). GPU 1 was color agnostic and GPU 2 color specific.

###### Foolish Assumption!
I predict individuals reading this article come from Computer Science and Mathematical backgrounds. Not requiring me to explain Mathematical notations step by step and also it was not the scope of this article.

Besides, the goal was to present the groundbreaking research in AlexNet and what is commonly missed by readers and misconceptions behind AlexNet overall. Further, it is written with limited information in common parts to encourage the readers to read the actual AlexNet paper.

**Original Paper:** https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks

**Original C/C++ Code:** https://code.google.com/archive/p/cuda-convnet/
