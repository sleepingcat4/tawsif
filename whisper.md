# Whisper-Explained: A Technical Dive into Radford’s Brilliance 

## Introduction
Before we dig deep regarding how Whisper is SOTA and so fascinating. I want to clarify that I am writing this from an advanced guy perspective, I have no interest in explaining all the code and architecture stepwise since it does not benefit me. 

Largely because, many Medium, Substack, and millions of LinkedIn people did that before me. I am interested in Whisper from a technical standpoint—why it is SOTA, why it matters, and why its predecessors are obsolete after it came out. 

I want to put caution: **If you are not a researcher, have no interest in research, or have reasons to study papers, this post is not targeted towards you.** Saying that, let’s start!

---

## Why did OpenAI make Whisper?
The reason is adequately defined in the abstract. OpenAI was looking for a way to transcribe large amounts of audio regardless of their quality. This is pretty much self-explanatory for large language model training. 

Yes, the silver bullet transformer’s capability extension wasn’t one of the reasons. (Very sorry to disappoint). 

OpenAI needed something that was zero-shot and could be translated to almost any domain of speech. Unlike past models restricted to music, speech, or niche fields, OpenAI wanted Whisper to extend and cover everything **out-of-the-box**.  

That being said, yes, OpenAI and everyone needed a solution.

---

## What was the solution?
**Whisper.**  
A model capable of transcribing pretty much any type of audio file out of the box with near human-level accuracy and without hallucinating in its guesses. 

---

## Fine, Why are we talking about Whisper? 
Past predecessors of Whisper had limited scope and stubborn improvements. Why? Because those papers thought answers always lay in either unsupervised or supervised training. 

Further, getting and processing large amounts of audio-text pairs for training is both expensive and challenging. Before Whisper, no robust and reliable solutions existed. Open corpora typically had no more than 5,000 hours of high-quality audio available. 

Whisper addressed all these problems and introduced something called a **weakly supervised trained model.**  

---

## Weakly Supervised Model
### Definition 
If we quote Radford directly:  
> **Weakly supervised training strategy** means you create a dataset based on whatever information and strategy you have in hand, such as rule-based heuristics, simple past models, pattern matching, and different criteria, and extract data from the large unbound Internet.

### Benefits of Weakly Supervised Training:
1. Models aren’t reliant on high-quality data inputs.  
2. Resilient to noise and change.  
3. Can compile large amounts of data for training.  
4. Better OOD (Out of Distribution) generalization.  

Radford argued that this strategy allows the model to achieve out-of-the-box robustness without wasting resources on fine-tuning or leveraging SFT (Supervised Fine-Tuning).  

---

## Interesting Pointers:
1. When weakly supervised training is scaled, **adding multi-tasking benefits model performance without downsides.**  
2. **Multilingualism** in weakly supervised training is actually beneficial.  

---

## Why don't people talk about Data Preprocessing?
Much like Alex of AlexNet, Radford also employed a strong data preprocessing strategy. Radford realized overprocessing data can make a model a parrot—it produces results only when fed high-quality data. 

### Steps Radford Took:
1. Relying on pre-made transcriptions without strictly enforcing high-quality transcription procedures.  
2. Removed the need for inverse text normalization.  
3. Excluded machine-generated transcriptions due to their impairing effect on performance.  
   - Identified machine-generated transcriptions by analyzing absence of punctuation and properties like all-uppercase or all-lowercase.  
4. Performed fuzzy deduplication.  

**Additional Measure:**  
The VoxLingua107 dataset was used to train an intermediate model to identify audio language. Poor-quality data was identified and removed by training initial models to detect bad data percentages.

---

## Why Train on No-Voice Data? 
It allowed the model to recognize the presence of voice and understand what silence between words feels like.  

**Fun Fact:**  
The name *Whisper* itself stands for something more meaningful:  
**WSPSR** = Web-scale Supervised Pretraining for Speech Recognition.

---

## Model Architecture
I won’t dive deep into every intricate detail of the Whisper model because it requires a two-part article. However, here are the key highlights:  

1. **V2 Training:** While V1 is about simplicity and understanding scale, V2 focuses on improving training and slightly modifying the architecture.  
2. **Transformer-based Model:** A simple transformer architecture does the heavy lifting. The real magic lies in the **feature extraction phase.**  
3. Whisper can be used for almost all audio activities. The architecture is designed for training custom tasks through tokens.  
4. **Conditioning Transcription Tasks:** Simple tokens control tasks without additional complexity.

---

## Training Concerns and Details (High-Level Explanation)
1. Unlike AlexNet’s augmentation-heavy approach, Radford opted for minimal augmentation, believing the model could learn from the true data and generalize well.  
2. Whisper theoretically attempts to predict the speaker, but due to limited data and 30s audio constraints, this behavior was excluded.

---

## Evaluation
Radford used **Word Error Rate (WER)** for evaluation but addressed its lack of robustness by standardizing texts before calculating WER.  

**Human Accuracy Comparison:**  
Whisper’s accuracy closely approaches human-level accuracy, with ideal accuracy represented as \( y = x \).  

---

## Scaling Laws for Whisper
While Whisper achieved SOTA, concerns about saturation and idiosyncrasies with large compute and data remain.  

**Key Findings:**
1. Increasing model size boosts performance, but inherent training data quality issues cause saturation before reaching human level.  
2. Performance follows a power law up to 54,000 hours, with diminishing returns beyond this point.

---

## Substantially Good Findings
Unanswered questions include:  
1. What happens if you train the encoder longer than the decoder (or vice versa)?  
2. Can combining Whisper’s encoder with a language model’s decoder yield better results?  

Radford or recent papers may have touched on these questions, but they remain exciting research directions.  

---

## Did I Leave Out Anything?
Yes, tons! Writing about Whisper could never fit into a single post, as Radford documented each step meticulously.  

I advise everyone to **read the original paper**.  
This post is part of my **Radford and Ilya Papers Series**, where I explore and explain their papers to myself.  

**Paper:** [Whisper: Web-scale Supervised Pretraining for Speech Recognition](https://arxiv.org/pdf/2212.04356)
